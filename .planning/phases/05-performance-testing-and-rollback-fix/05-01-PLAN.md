---
phase: 05-performance-testing-and-rollback-fix
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - upstream/tests_performance.py
  - .github/workflows/ci.yml
autonomous: true

must_haves:
  truths:
    - "Locust tests cover realistic API usage scenarios (list, filter, search, detail)"
    - "Performance tests run in CI and fail if p95 exceeds 500ms"
    - "Load tests authenticate using JWT tokens"
    - "Tests include realistic wait times between requests"
  artifacts:
    - path: "upstream/tests_performance.py"
      provides: "Locust performance test suite"
      min_lines: 100
    - path: ".github/workflows/ci.yml"
      provides: "CI workflow with performance test step"
      contains: "locust"
  key_links:
    - from: "upstream/tests_performance.py"
      to: "/api/v1/*"
      via: "Locust HttpUser tasks"
      pattern: "self\\.client\\.(get|post)"
    - from: ".github/workflows/ci.yml"
      to: "upstream/tests_performance.py"
      via: "locust command"
      pattern: "locust.*tests_performance"
---

<objective>
Create Locust performance test suite and integrate into CI pipeline

Purpose: Validate API performance meets SLA targets (p95 < 500ms) under realistic load conditions
Output: Performance test file and updated CI workflow with automated performance checks
</objective>

<execution_context>
@/home/codespace/.claude/get-shit-done/workflows/execute-plan.md
@/home/codespace/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@upstream/tests_api.py (reference for API endpoints and authentication patterns)
@.github/workflows/ci.yml (existing CI workflow to extend)
@requirements.txt (confirms locust is available)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Locust performance test suite</name>
  <files>upstream/tests_performance.py</files>
  <action>
Create a Locust performance test file with the following:

1. **Authentication setup:**
   - Create `on_start` method that obtains JWT token via POST to `/api/v1/auth/token/`
   - Store access token for subsequent requests
   - Use test credentials from test fixtures (username="user_a", password="testpass123")

2. **User behavior class (HttpUser subclass):**
   - Set `wait_time = between(1, 3)` for realistic pacing
   - Set `host` to be configurable (default: "http://localhost:8000")

3. **Task methods with weights (7+ tasks):**
   - `list_claims` (weight=5): GET /api/v1/claims/
   - `get_claim_detail` (weight=3): GET /api/v1/claims/{id}/ (use known ID or skip if none)
   - `filter_claims_by_payer` (weight=3): GET /api/v1/claims/?payer=test
   - `search_claims` (weight=2): GET /api/v1/claims/?search=99213
   - `get_payer_summary` (weight=4): GET /api/v1/claims/payer_summary/
   - `list_drift_events` (weight=3): GET /api/v1/drift-events/
   - `filter_drift_by_severity` (weight=2): GET /api/v1/drift-events/?min_severity=0.5
   - `get_dashboard` (weight=4): GET /api/v1/dashboard/

4. **Performance targets (in docstring):**
   - p95 < 500ms for list endpoints
   - p95 < 200ms for detail endpoints
   - Error rate < 1%

5. **Response validation:**
   - Check status codes (200 expected for authenticated requests)
   - Mark failed requests appropriately

Note: Do NOT use deprecated `task()` decorator syntax. Use `@task(weight)` decorator properly.
  </action>
  <verify>
Run: `python -c "from upstream.tests_performance import UpstreamUser; print('Locust tests import OK')"`
Verify file has at least 7 task methods with @task decorator
  </verify>
  <done>
Locust test file exists with 7+ realistic API tasks, JWT authentication, and documented performance targets
  </done>
</task>

<task type="auto">
  <name>Task 2: Add performance test step to CI workflow</name>
  <files>.github/workflows/ci.yml</files>
  <action>
Add a performance test job to the existing CI workflow:

1. **Add new job `performance` that runs after `test` job:**
   ```yaml
   performance:
     runs-on: ubuntu-latest
     needs: test  # Only run if tests pass

     steps:
     - name: Checkout code
       uses: actions/checkout@v4

     - name: Set up Python 3.12
       uses: actions/setup-python@v5
       with:
         python-version: '3.12'
         cache: 'pip'

     - name: Install system dependencies
       run: |
         sudo apt-get update
         sudo apt-get install -y libcairo2 libpango-1.0-0 libpangocairo-1.0-0 libgdk-pixbuf2.0-0 shared-mime-info

     - name: Install Python dependencies
       run: |
         python -m pip install --upgrade pip
         pip install -r requirements.txt

     - name: Set up environment
       run: |
         cp .env.example .env
         echo "DEBUG=True" >> .env
         # Generate random key for CI environment
         python -c "import secrets; print(f'SECRET_KEY={secrets.token_hex(32)}')" >> .env

     - name: Run migrations and create test data
       run: |
         python manage.py migrate --noinput
         python manage.py shell -c "
         from django.contrib.auth.models import User
         from upstream.models import Customer, UserProfile
         customer = Customer.objects.create(name='Test Customer')
         user = User.objects.create_user('user_a', 'user_a@test.com', 'testpass123')
         UserProfile.objects.create(user=user, customer=customer)
         print('Test user created')
         "

     - name: Start Django server
       run: |
         python manage.py runserver 0.0.0.0:8000 &
         sleep 5

     - name: Run Locust performance tests
       run: |
         locust -f upstream/tests_performance.py \
           --headless \
           -u 5 \
           -r 1 \
           -t 30s \
           --host http://127.0.0.1:8000 \
           --csv=perf_results \
           --only-summary
       continue-on-error: true

     - name: Check performance thresholds
       run: |
         # Parse CSV results and check p95 < 500ms
         python -c "
         import csv
         import sys

         try:
             with open('perf_results_stats.csv', 'r') as f:
                 reader = csv.DictReader(f)
                 failures = []
                 for row in reader:
                     if row['Name'] == 'Aggregated':
                         p95 = float(row['95%']) if row['95%'] else 0
                         error_rate = float(row['Failure Count']) / max(float(row['Request Count']), 1) * 100

                         if p95 > 500:
                             failures.append(f'p95 latency {p95}ms exceeds 500ms threshold')
                         if error_rate > 5:
                             failures.append(f'Error rate {error_rate:.1f}% exceeds 5% threshold')

                 if failures:
                     print('Performance check FAILED:')
                     for f in failures:
                         print(f'  - {f}')
                     sys.exit(1)
                 else:
                     print('Performance check PASSED')
         except FileNotFoundError:
             print('No performance results found - tests may have failed to run')
             sys.exit(1)
         "

     - name: Upload performance results
       uses: actions/upload-artifact@v4
       if: always()
       with:
         name: performance-results
         path: perf_results*.csv
   ```

2. **Important notes:**
   - Use `needs: test` to ensure tests pass first
   - Use `continue-on-error: true` for locust command to allow threshold check to run
   - The threshold check script parses CSV output and fails if p95 > 500ms or error rate > 5%
   - Upload results as artifact for debugging
  </action>
  <verify>
Run: `python -c "import yaml; yaml.safe_load(open('.github/workflows/ci.yml')); print('CI workflow YAML valid')"`
Verify workflow contains job named "performance" with locust command
  </verify>
  <done>
CI workflow includes performance test job that runs Locust and fails on SLA violations
  </done>
</task>

</tasks>

<verification>
1. `python -c "from upstream.tests_performance import UpstreamUser"` imports successfully
2. `grep -c "@task" upstream/tests_performance.py` returns 7 or more
3. `grep "locust" .github/workflows/ci.yml` shows locust command in workflow
4. `python -c "import yaml; yaml.safe_load(open('.github/workflows/ci.yml'))"` validates YAML syntax
</verification>

<success_criteria>
- Locust test file exists with 7+ task methods covering key API endpoints
- Tests use JWT authentication matching existing test patterns
- CI workflow has performance job that:
  - Runs after test job passes
  - Spins up Django server with test data
  - Executes Locust in headless mode
  - Checks p95 < 500ms threshold
  - Uploads results as artifact
</success_criteria>

<output>
After completion, create `.planning/phases/05-performance-testing-and-rollback-fix/05-01-SUMMARY.md`
</output>
